{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST Dataset has 70000 instances of hand written digits from 0-9, each image size is 28*28\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for training\n",
    "#Convert to Float\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "#Flatten images to 1-D vector of 784 features (28*28).\n",
    "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
    "\n",
    "#One hot encoding of labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "#Normalize images value from [0, 255] to [0, 1].\n",
    "x_train = x_train / 255.\n",
    "x_test =  x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(activation, initializer):\n",
    "      \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(512, input_dim = 784, activation = activation, kernel_initializer = initializer))\n",
    "    model.add(Dense(256, activation = activation, kernel_initializer = initializer))\n",
    "    model.add(Dense(128, activation = activation, kernel_initializer = initializer))\n",
    "    model.add(Dense(64, activation = activation, kernel_initializer = initializer))\n",
    "    model.add(Dense(10, activation = 'softmax', kernel_initializer = initializer))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(), metrics = ['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal_model = build_model(activation = 'relu', initializer = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.01))\n",
    "random_normal_hist = random_normal_model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal_model1 = build_model(activation = 'relu', initializer = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.1))\n",
    "random_normal_hist1 = random_normal_model1.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal_model2 = build_model(activation = 'relu', initializer = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 1.0))\n",
    "random_normal_hist2 = random_normal_model2.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_uniform_model = build_model(activation = 'relu', initializer = tf.keras.initializers.RandomUniform(minval = -1, maxval = 1))\n",
    "random_uniform_hist = random_uniform_model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_uniform_model1 = build_model(activation = 'relu', initializer = tf.keras.initializers.RandomUniform(minval = 0, maxval = 1))\n",
    "random_uniform_hist1 = random_uniform_model1.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_model = build_model(activation = 'relu', initializer = tf.keras.initializers.Ones())\n",
    "ones_hist = ones_model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_model = build_model(activation = 'relu', initializer = tf.keras.initializers.Zeros())\n",
    "zeros_hist = zeros_model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glorot_normal_model = build_model(activation = 'relu', initializer = tf.keras.initializers.GlorotNormal())\n",
    "glorot_normal_hist = model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glorot_uniform_model1 = build_model(activation = 'relu', initializer = tf.keras.initializers.GlorotUniform())\n",
    "glorot_uniform_hist1 = model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_normal_model = build_model(activation = 'relu', initializer = tf.keras.initializers.HeNormal())\n",
    "he_normal_hist = model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_uniform_model = build_model(activation = 'relu', initializer = tf.keras.initializers.HeUniform())\n",
    "he_uniform_hist = model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecun_uniform_model = build_model(activation = 'relu', initializer = tf.keras.initializers.LecunUniform())\n",
    "lecun_uniform_hist = model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecun_normal_model = build_model(activation = 'relu', initializer = tf.keras.initializers.LecunNormal())\n",
    "lecun_normal_hist = model.fit(x_train, y_train, epochs = 15, validation_data = (x_test, y_test), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize = (12, 28))\n",
    "ax[0, 0].plot(random_normal_hist.history['acc'])\n",
    "ax[0, 0].plot(random_normal_hist.history['val_acc'])\n",
    "ax[0, 0].set_ylabel('Accuracy')\n",
    "ax[0, 0].set_xlabel('Epochs')\n",
    "ax[0, 0].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[0, 0].set_title('Random Normal - StdDev of 0.05')\n",
    "\n",
    "ax[0, 1].plot(random_normal_hist.history['loss'])\n",
    "ax[0, 1].plot(random_normal_hist.history['val_loss'])\n",
    "ax[0, 1].set_ylabel('Loss')\n",
    "ax[0, 1].set_xlabel('Epoch')\n",
    "ax[0, 1].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[0, 1].set_title('Random Normal - StdDev of 0.05')\n",
    "\n",
    "ax[1, 0].plot(random_normal_hist1.history['acc'])\n",
    "ax[1, 0].plot(random_normal_hist1.history['val_acc'])\n",
    "ax[1, 0].set_ylabel('Accuracy')\n",
    "ax[1, 0].set_xlabel('Epochs')\n",
    "ax[1, 0].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[1, 0].set_title('Random Normal - StdDev of 0.1')\n",
    "\n",
    "ax[1, 1].plot(random_normal_hist1.history['loss'])\n",
    "ax[1, 1].plot(random_normal_hist1.history['val_loss'])\n",
    "ax[1, 1].set_ylabel('Loss')\n",
    "ax[1, 1].set_xlabel('Epoch')\n",
    "ax[1, 1].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[1, 1].set_title('Random Normal - StdDev of 0.1')\n",
    "\n",
    "ax[2, 0].plot(random_normal_hist2.history['acc'])\n",
    "ax[2, 0].plot(random_normal_hist2.history['val_acc'])\n",
    "ax[2, 0].set_ylabel('Accuracy')\n",
    "ax[2, 0].set_xlabel('Epochs')\n",
    "ax[2, 0].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[2, 0].set_title('Random Normal - StdDev of 1.0')\n",
    "\n",
    "ax[2, 1].plot(random_normal_hist2.history['loss'])\n",
    "ax[2, 1].plot(random_normal_hist2.history['val_loss'])\n",
    "ax[2, 1].set_ylabel('Loss')\n",
    "ax[2, 1].set_xlabel('Epoch')\n",
    "ax[2, 1].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[2, 1].set_title('Random Normal - StdDev of 1.0')\n",
    "\n",
    "ax[3, 0].plot(random_uniform_hist.history['acc'])\n",
    "ax[3, 0].plot(random_uniform_hist.history['val_acc'])\n",
    "ax[3, 0].set_ylabel('Accuracy')\n",
    "ax[3, 0].set_xlabel('Epochs')\n",
    "ax[3, 0].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[3, 0].set_title('Random Uniform - [-1, 1]')\n",
    "\n",
    "ax[3, 1].plot(random_uniform_hist.history['loss'])\n",
    "ax[3, 1].plot(random_uniform_hist.history['val_loss'])\n",
    "ax[3, 1].set_ylabel('Loss')\n",
    "ax[3, 1].set_xlabel('Epoch')\n",
    "ax[3, 1].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[3, 1].set_title('Random Uniform - [-1, 1]')\n",
    "\n",
    "ax[4, 0].plot(random_uniform_hist1.history['acc'])\n",
    "ax[4, 0].plot(random_uniform_hist1.history['val_acc'])\n",
    "ax[4, 0].set_ylabel('Accuracy')\n",
    "ax[4, 0].set_xlabel('Epochs')\n",
    "ax[4, 0].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[4, 0].set_title('Random Uniform - [0, 1]')\n",
    "\n",
    "ax[4, 1].plot(random_uniform_hist1.history['loss'])\n",
    "ax[4, 1].plot(random_uniform_hist1.history['val_loss'])\n",
    "ax[4, 1].set_ylabel('Loss')\n",
    "ax[4, 1].set_xlabel('Epoch')\n",
    "ax[4, 1].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[4, 1].set_title('Random Uniform - [0, 1]')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize = (12, 10))\n",
    "ax[0, 0].plot(ones_hist.history['acc'])\n",
    "ax[0, 0].plot(ones_hist.history['val_acc'])\n",
    "ax[0, 0].set_ylabel('Accuracy')\n",
    "ax[0, 0].set_xlabel('Epochs')\n",
    "ax[0, 0].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[0, 0].set_title('Ones')\n",
    "\n",
    "ax[0, 1].plot(ones_hist.history['loss'])\n",
    "ax[0, 1].plot(ones_hist.history['val_loss'])\n",
    "ax[0, 1].set_ylabel('Loss')\n",
    "ax[0, 1].set_xlabel('Epoch')\n",
    "ax[0, 1].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[0, 1].set_title('Ones')\n",
    "\n",
    "ax[1, 0].plot(zeros_hist.history['acc'])\n",
    "ax[1, 0].plot(zeros_hist.history['val_acc'])\n",
    "ax[1, 0].set_ylabel('Accuracy')\n",
    "ax[1, 0].set_xlabel('Epochs')\n",
    "ax[1, 0].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[1, 0].set_title('Zeros')\n",
    "\n",
    "ax[1, 1].plot(zeros_hist.history['loss'])\n",
    "ax[1, 1].plot(zeros_hist.history['val_loss'])\n",
    "ax[1, 1].set_ylabel('Loss')\n",
    "ax[1, 1].set_xlabel('Epoch')\n",
    "ax[1, 1].legend(['Train', 'Test'], loc='upper left')\n",
    "ax[1, 1].set_title('Zeros')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given below is a method to manually initialize and have complete control (If necessary and for custom weight init as required) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over the layers of a given model\n",
    "for layer in model.layers:\n",
    "    \n",
    "    if isinstance(layer, tf.keras.layers.Dense):\n",
    "        \n",
    "        #Initialize weights in the range of [-y, y], where y = 1/âˆšn (n is no. of neurons in the next layer)\n",
    "        \n",
    "        shape = (layer.weights[0].shape[0], layer.weights[0].shape[1])\n",
    "        y = 1.0/np.sqrt(shape[0])\n",
    "        \n",
    "        rule_weights = np.random.uniform(-y, y, shape)\n",
    "        \n",
    "        #Weights\n",
    "        layer.weights[0] = rule_weights \n",
    "        \n",
    "        #Bias\n",
    "        layer.weights[1] = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can visualize layer weight init distribution in a graphical way also, using Tensorboard - Head over to the notebook [Tensorboard Demo]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the above given weight init techniques, feel free to change and run the respective init techniques, make slight changes to the matplotlib cell to accomodate your interested weight init graph for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can combine various weight init in a single graph also, for a visualization and comparision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
